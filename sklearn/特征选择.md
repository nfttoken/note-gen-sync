# scikit-learn 特征选择的八种方法

特征选择是机器学习中的关键步骤，涉及从数据集中选择最相关的特征。通过消除不相关或冗余的特征，特征选择技术可以提高模型性能和效率。在本指南中，我们将探索一些常见的特征选择技术，并使用 Boston Housing 数据集提供代码示例。

Boston Housing 数据集包含有关波士顿房价的信息。它由各种特征组成，例如每套住宅的平均房间数、犯罪率和师生比例。我们的目标是选择对预测房价影响最大的特征子集。

## 1、单变量特征选择Univariate Feature Selection

单变量特征选择根据统计检验单独评估每个特征，以衡量每个特征与目标变量之间的相关性。让我们使用条形图可视化特征分数。

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Perform univariate feature selection
selector = SelectKBest(score_func=f_regression, k=5)
X_new = selector.fit_transform(X, y)

# Get the selected feature indices
selected_indices = selector.get_support(indices=True)
selected_features = data.feature_names[selected_indices]

# Get the feature scores
scores = selector.scores_

# Plot the feature scores
plt.figure(figsize=(10, 6))
plt.bar(range(len(data.feature_names)), scores, tick_label=data.feature_names)
plt.xticks(rotation=90)
plt.xlabel('Features')
plt.ylabel('Scores')
plt.title('Univariate Feature Selection: Feature Scores')
plt.show()

print("Selected Features:")
print(selected_features)
```

选出的特征为： ['INDUS' 'RM' 'TAX' 'PTRATIO' 'LSTAT']

在本例中，我们使用f\_regression得分函数选择前 5 个特征，并使用条形图可视化特征得分。所选特征是与目标变量相关性最高的特征。如果我们有一个分类目标而不是一个连续目标，我们可能会使用 chi2 而不是使用 f\_regression。

![所选要素重要性](https://res.cloudinary.com/dayqxxsip/image/upload/v1692207922/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/ufs_su8uz2.png)

## 2、递归特征消除 Recursive Feature Elimination（RFE）

递归特征消除 （RFE） 是一种迭代方法，从所有特征开始，然后根据模型的性能递归消除最不重要的特征。让我们使用折线图可视化特征排名。

```python
import matplotlib.pyplot as plt
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Perform Recursive Feature Elimination
estimator = LinearRegression()
selector = RFE(estimator, n_features_to_select=5)
X_new = selector.fit_transform(X, y)

# Get the selected feature indices
selected_indices = selector.get_support(indices=True)
selected_features = data.feature_names[selected_indices]

# Get the feature rankings
rankings = selector.ranking_

# Plot the feature rankings
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(rankings) + 1), rankings, marker='o')
plt.xticks(range(1, len(rankings) + 1), data.feature_names, rotation=90)
plt.xlabel('Features')
plt.ylabel('Rankings')
plt.title('Recursive Feature Elimination: Feature Rankings')
plt.show()

print("Selected Features:")
print(selected_features)
```

选出的特征为： ['CHAS' 'NOX' 'RM' 'DIS' 'PTRATIO']

在这里，我们使用 LinearRegression 作为估计器，并选择前 5 个特征。我们使用折线图可视化特征排名。等级越低表示功能越重要。

![所选要素重要性](https://res.cloudinary.com/dayqxxsip/image/upload/v1692207923/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/rfe_pfcvvx.png)

## 3、L1 正则化 L1 Regularisation（套索）

L1 正则化，也称为套索正则化，对线性回归模型应用惩罚项，鼓励稀疏特征权重。这导致某些特征权重被驱动为零，从而有效地仅选择最相关的特征。让我们使用水平条形图可视化特征系数。

```python
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Perform L1 regularisation (Lasso)
lasso = Lasso(alpha=0.1)
lasso.fit(X, y)

# Get the non-zero feature coefficients
nonzero_coefs = lasso.coef_
selected_indices = nonzero_coefs != 0
selected_features = data.feature_names[selected_indices]
nonzero_coefs = nonzero_coefs[selected_indices]

# Plot the feature coefficients
plt.figure(figsize=(10, 6))
plt.barh(range(len(nonzero_coefs)), nonzero_coefs, tick_label=selected_features)
plt.xlabel('Coefficient Values')
plt.ylabel('Features')
plt.title('L1 Regularisation (Lasso): Feature Coefficients')
plt.show()

print("Selected Features:")
print(selected_features)
```

选出的特征为： ['CRIM' 'ZN' 'INDUS' 'CHAS' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']

在本例中，我们应用正则化强度 （alpha） 为 0.1 的 L1 正则化。我们使用水平条形图可视化非零特征系数。所选特征是 Lasso 模型中具有非零系数的特征。

![所选要素重要性](https://res.cloudinary.com/dayqxxsip/image/upload/v1692207923/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/l1_skucse.png)

## 4、基于树的方法 tree-based methods

基于树的方法，如随机森林和梯度提升，本质上是通过评估每个特征在树构建过程中的重要性来执行特征选择。让我们使用条形图可视化特征重要性。

```python
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Perform feature selection using Random Forest
forest = RandomForestRegressor(n_estimators=100)
forest.fit(X, y)

# Get feature importances
importances = forest.feature_importances_

# Sort feature importances in descending order
sorted_indices = importances.argsort()[::-1]

# Select the top k features
k = 5
selected_features = data.feature_names[sorted_indices[:k]]
top_importances = importances[sorted_indices[:k]]

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.bar(range(len(top_importances)), top_importances, tick_label=selected_features)
plt.xticks(rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Tree-Based Methods: Feature Importances')
plt.show()

print("Selected Features:")
print(selected_features)
```

选出的特征为： ['RM' 'LSTAT' 'DIS' 'CRIM' 'NOX']

在本例中，我们使用具有 100 个估计器的随机森林模型来计算特征重要性。我们根据重要性得分选择前 5 个特征，并使用条形图对其进行可视化。

![所选要素重要性](https://res.cloudinary.com/dayqxxsip/image/upload/v1692207923/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/tree_zvx2cl.png)

## 5、主成分分析 Principal Component Analysis（PCA）

主成分分析 （PCA） 是一种降维技术，可将原始特征转换为一组新的不相关变量，称为主成分。让我们使用条形图可视化解释的方差比。

```python
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Perform PCA
pca = PCA(n_components=5)
X_new = pca.fit_transform(X)

# Get the explained variance ratio
explained_variance = pca.explained_variance_ratio_

# Plot the explained variance ratio
plt.figure(figsize=(10, 6))
plt.bar(range(1, len(explained_variance) + 1), explained_variance)
plt.xlabel('Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Principal Component Analysis (PCA): Explained Variance Ratio')
plt.show()

# Get the loadings (principal component vectors)
loadings = pca.components_

# Create a loading plot
plt.figure(figsize=(10, 6))
for i, (loading, feature_name) in enumerate(zip(loadings, data.feature_names)):
    plt.arrow(0, 0, loading[0], loading[1], head_width=0.05, head_length=0.1, fc='blue', ec='blue')
    plt.text(loading[0], loading[1], feature_name, fontsize=12, ha='center', va='center', color='black')
plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.8)
plt.axvline(x=0, color='gray', linestyle='--', linewidth=0.8)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Loading Plot: Feature Contributions to Principal Components')
plt.grid(True)
plt.show()
```

选出的特征为： ['CHAS' 'INDUS' 'CRIM', 'ZN', 'NOX']

在此示例中，我们选择捕获数据方差最大的前 5 个主成分。我们使用 5 个主成分的条形图和具有 2 个主成分的二维空间汇总来可视化这些成分的解释方差比，以查看载荷/特征重要性。

![PCA 解释方差](https://res.cloudinary.com/dayqxxsip/image/upload/v1692207922/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/pca_yoiumv.png)

PCA 解释方差

![特征重要性的 PCA 载荷](https://res.cloudinary.com/dayqxxsip/image/upload/v1692207922/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/pca-loadings_aul1re.png)

在条形图的 PCA 示例中，变量的重要性不像特征重要性图那样由条形高度直接表示。相反，PCA 专注于将原始特征转换为一组新的不相关变量，称为主成分。这些主成分是原始特征的线性组合，并按它们捕获的方差量降序排序。

PCA 示例中的解释方差比图显示了每个主成分解释的数据集中总方差的比例。虽然该图并未直接指示哪些原始特征最重要，但它确实有助于我们了解每个主成分对数据变异性的总体贡献。

通常，当您执行 PCA 时，前几个主成分往往会捕获数据集中的大部分方差。因此，在解释数据集的变异性方面，对这些早期主成分贡献最大的原始特征可以被认为更为重要。

然而，由于主成分的线性组合性质，确定哪些特定的原始特征对特定主成分的贡献最大可能具有挑战性。如果需要了解原始特征与特定主成分之间的关系，则可能需要进行进一步的分析，例如查看主成分的载荷，这些载荷代表每个原始特征对主成分构造的贡献。

总之，在 PCA 分析中，重点更多地在于了解变量之间的变异性和关系，而不是像其他特征选择方法那样直接识别“最重要”的变量。

## 6、基于关联的特征选择 Correlation-based Feature Selection

基于相关性的特征选择衡量每个特征与目标变量之间的相关性，以及不同特征之间的相关性。让我们使用热图可视化特征相关性。

```python
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Calculate feature correlations with target variable
correlations = np.abs(np.corrcoef(X.T, y)[:X.shape[1], -1])
sorted_indices = correlations.argsort()[::-1]

# Select the top k features
k = 5
selected_features = data.feature_names[sorted_indices[:k]]
top_correlations = correlations[sorted_indices[:k]]

print("Selected Features with correlation:")
print(selected_features)
print(top_correlations)

```

具有相关性的选定特征：

['LSTAT' 'RM' 'PTRATIO' 'INDUS' 'TAX']

[0.73766273 0.69535995 0.50778669 0.48372516 0.46853593]

在本例中，我们计算每个特征与目标变量之间的绝对相关性。我们选择与目标变量 y 相关性最高的前 5 个特征。

## 7、互信息 Mutual Information

互信息衡量两个变量之间的统计依赖性。在特征选择的上下文中，它量化了一个特征提供的有关目标变量的信息量。让我们使用条形图可视化特征分数。

```python
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, mutual_info_regression
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Perform mutual information feature selection
selector = SelectKBest(score_func=mutual_info_regression, k=5)
X_new = selector.fit_transform(X, y)

# Get the selected feature indices
selected_indices = selector.get_support(indices=True)
selected_features = data.feature_names[selected_indices]

# Get the feature scores
scores = selector.scores_

# Plot the feature scores
plt.figure(figsize=(10, 6))
plt.bar(range(len(data.feature_names)), scores, tick_label=data.feature_names)
plt.xticks(rotation=90)
plt.xlabel('Features')
plt.ylabel('Scores')
plt.title('Mutual Information: Feature Scores')
plt.show()

print("Selected Features:")
print(selected_features)
```

选出的特征为： ['INDUS' 'NOX' 'RM' 'PTRATIO' 'LSTAT']

在本例中，我们使用mutual_info_regression得分函数根据互信息得分选择前 5 个特征。我们使用条形图可视化特征分数。此方法也适用于具有分类目标的数据集，但不要像导入和使用“mutual_info_classif”那样使用“mutual_info_regression”。`score_func`

![所选要素重要性](https://res.cloudinary.com/dayqxxsip/image/upload/v1692208757/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/mutual_iu2rbc.png)

## 8、顺序特征选择 Sequential Feature Selection

顺序特征选择是一种组合多个特征子集并使用机器学习模型评估其性能的方法。让我们使用折线图可视化特征性能。

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X = data.data
y = data.target

# Perform sequential feature selection
estimator = LinearRegression()
selector = SequentialFeatureSelector(estimator, n_features_to_select=5, direction='forward')
selector.fit(X, y)

# Get the selected feature indices
selected_indices = np.where(selector.support_)[0]
selected_features = data.feature_names[selected_indices]

# Get the feature performance (manually store performance scores)
performance = []

for step in range(1, len(selected_indices) + 1):
    subset_indices = selected_indices[:step]
    X_subset = X[:, subset_indices]
    score = -np.mean(np.abs(np.mean(LinearRegression().fit(X_subset, y).predict(X_subset) - y)))
    performance.append(score)

# Plot the feature performance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(performance) + 1), performance, marker='o')
plt.xticks(range(1, len(performance) + 1), selected_features, rotation=90)
plt.xlabel('Features')
plt.ylabel('Performance')
plt.title('Sequential Feature Selection: Feature Performance')
plt.show()

print("Selected Features:")
print(selected_features)
```

选出的特征为： ['CRIM' 'CHAS' 'RM' 'PTRATIO' 'LSTAT']

在本例中，我们使用 LinearRegression 作为估计量，并使用前向选择方法选择前 5 个特征。我们使用折线图可视化特征性能。

![所选要素重要性](https://res.cloudinary.com/dayqxxsip/image/upload/v1692207922/App%20Images/Blog%20Images/Article%20Images/Feature%20Selection/sequential_fqdily.png)

## 结论

总之，特征选择技术对于通过选择最相关的特征和降维来改进机器学习模型至关重要。在本指南中，我们探索了各种技术，并将它们应用于波士顿住房数据集。

通过将这些特征选择技术合并到机器学习工作流程中，您可以增强模型性能、减少过度拟合并更好地了解底层数据模式。考虑尝试不同的技术并评估它们对特定数据集和任务的影响，以确定最有效的特征子集。最终，特征选择使您能够构建更强大、可解释的模型，从而提供准确的预测和有价值的见解。

## 辨析：**Correlation-based Feature Selection**（基于相关性的特征选择）和 **Univariate Feature Selection**（单变量特征选择）的差异

**1. **定义与核心思想*

 **Correlation-based Feature Selection（基于相关性的特征选择）**

* 核心思想：通过计算特征与目标变量之间的相关性（或特征之间的相关性），选择那些与目标变量相关性较高、同时避免特征之间冗余（高相关性）的特征。
* 通常使用相关性系数（如皮尔逊相关系数、斯皮尔曼相关系数等）来评估特征与目标变量或特征之间的关系。

* 主要目标：减少特征冗余（如移除高度相关的特征），提高模型的泛化能力。

**Univariate Feature Selection（单变量特征选择）**


* 核心思想：通过对每个特征单独与目标变量的关系进行统计检验，评估特征的重要性，并选择得分最高的特征。
* 常见方法包括：方差分析（ANOVA F-test）、卡方检验（Chi-squared test）、互信息法（Mutual Information）等。
* 主要目标：选择与目标变量有显著统计关系的特征，忽略特征之间的关系。

---

2. **实现方式*** **Correlation-based Feature Selection**

* 通常不直接由 sklearn 的内置方法实现，而是通过手动计算相关性矩阵（例如使用 pandas.DataFrame.corr()）或结合其他工具完成。
* 步骤：

  1. 计算特征与目标变量的相关性（如皮尔逊相关系数）。
  2. 剔除与目标变量相关性较低的特征。
  3. 可选：**检查特征之间的相关性，移除高度相关的特征对**（例如相关系数 > 0.8）。
* **示例代码：**
  **python**

  ```python
  import pandas as pd
  from sklearn.datasets import make_regression

  # 假设 X 是特征矩阵，y 是目标变量
  X, y = make_regression(n_features=10, random_state=42)
  df = pd.DataFrame(X)
  df['target'] = y

  # 计算相关性矩阵
  corr_matrix = df.corr()
  # 选择与目标变量相关性高的特征
  target_corr = corr_matrix['target'].abs().sort_values(ascending=False)
  selected_features = target_corr[target_corr > 0.5].index.drop('target')
  ```
* **Univariate Feature Selection**

  * sklearn 提供了 **SelectKBest**、SelectPercentile 等工具，通过统计检验方法（如 F-test、Chi-squared、互信息）来选择特征。
  * 步骤：

    1. 对每个特征单独计算与目标变量的统计得分（如 F 值、卡方值等）。
    2. 根据得分选择前 k 个或前 p% 的特征。
  * **示例代码：**

    ```python
    from sklearn.feature_selection import SelectKBest, f_classif
    from sklearn.datasets import make_classification

    # 假设 X 是特征矩阵，y 是目标变量
    X, y = make_classification(n_features=10, random_state=42)

    # 使用 ANOVA F-test 选择前 5 个特征
    selector = SelectKBest(score_func=f_classif, k=5)
    X_new = selector.fit_transform(X, y)

    # 查看选择的特征索引
    selected_indices = selector.get_support(indices=True)
    print("Selected feature indices:", selected_indices)
    ```

---

3. 主要区别

| 特性           | 相关性correlation                        | 单变量univarate                              |
| -------------- | ---------------------------------------- | -------------------------------------------- |
| 特征间关系     | 考虑特征之间的相关性（可剔除冗余特征）   | 不考虑特征之间的关系，仅评估特征与y的关系    |
| 实现方法       | pandas.DataFrame.corr()                  | sklearn.feature_selection.SelectKBest        |
| 优点、适用场景 | 适合需要减少特征冗余的场景（如线性模型） | 适合快速筛选与y强相关的特征，尤其是X非常多时 |
| 缺点           | 基于线性关系的假设                       | 忽略特征之间的交互作用，可能选择到冗余特征   |


6. **总结*** **Correlation-based Feature Selection** 更关注特征与目标变量以及特征之间的相关性，适合需要减少冗余特征的场景，但实现较复杂，依赖线性假设。

* **Univariate Feature Selection** 更简单高效，适合快速筛选特征，但不考虑特征间的关系，可能导致冗余特征被保留。
* **如果你需要兼顾特征间关系和与目标变量的相关性，可以结合两者：先用单变量特征选择筛选出重要特征，再用相关性分析剔除冗余特征。**

原文链接：[使用 scikit-learn 执行特征选择的八种方法 |代码的棚卸量](https://www.shedloadofcode.com/blog/eight-ways-to-perform-feature-selection-with-scikit-learn)
